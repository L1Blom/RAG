# Constants ini file for unit testing DO NOT REMOVE!!
# Example constants ini file, to be parsed from <your project dir>/constants by configparser

[DEFAULT]
# simple string like "mydata"
ID = template
description = Template project
# Config server at same host to get API and PORT
CONFIG_SERVER = http://localhost:8000
# Any other level will make it less verbose
LOGGING_LEVEL = INFO
# Directory that will be scanned for files to be added to the context
DATA_DIR=data/template
HTML=data/template # please keep the same as DATA_DIR, for legacy reasons
# All the file extentions you want to be part of the context, see LangChain documentation
DATA_GLOB_TXT = *.txt
DATA_GLOB_PDF = *.pdf
DATA_GLOB_DOCX = *.docx
DATA_GLOB_XLSX = *.xlsx
DATA_GLOB_PPTX = *.pptx
DATA_GLOB_MD = *.md
DATA_GLOB_HTML = *.html
DATA_GLOB_X = *.x
# Persistence directory for vectorstore
PERSISTENCE = data/template/vectorstore
# the lower, the more precise. TODO can be changed dynamically
TEMPERATURE = 0.0
# Similir search number of hits
SIMILAR = 4
# Max tokens to be sued by LLM
max_tokens = 4096
# Influences the way answers are produced
contextualize_q_system_prompt = Given a chat history and the latest user question 
    which might reference context in the chat history formulate a standalone question 
    which can be understood without the chat history.
# Influences the way answers are produced
system_prompt = You are a chatbot
# depending on your data
chunk_size=500    
# idem
chunk_overlap=50  
# score threshold
score=0.1
# search type: similarity_score_threshold or mmd or similarity 
search_type=similarity_score_threshold

[X]
# X (Twitter) API Configuration
# Set your X API Bearer Token in environment variable: X_API_KEY or TWITTER_BEARER_TOKEN
# Get your API key from https://developer.twitter.com/

[FLASK]
# any unused port, will run the Flask server
PORT = 5000
# Set to True if you want to debug
DEBUG = False
# max files size in MB for upload
MAX_MB_SIZE = 512

[LLMS]
# Possible LLMs
LLMS = OPENAI,GROQ,OLLAMA,AZURE,NEBUL

# LLM to be used
#USE_LLM = OLLAMA
USE_LLM = OPENAI
#USE_LLM = AZURE
#USE_LLM = GROQ
#USE_LLM = NEBUL
# The LLM to be used as listed in OpenAI

[LLMS.OPENAI]
MODELTEXT = gpt-4o
EMBEDDING_MODEL = text-embedding-3-small 

[LLMS.AZURE]
modeltext = gpt-4o
embedding_model = text-embedding-3-large

[LLMS.AZURE.OPENAI]
model_endpoint = https://chat-interactions-proxy-3.openai.azure.com/openai/deployments/
embedding_endpoint = https://chat-interactions-proxy-3.openai.azure.com/openai/deployments/
model_api_version = 2025-01-01-preview
embedding_api_version = 2023-05-15
models = gpt-4o,gpt-5-mini
embeddings = text-embedding-3-large,text-embedding-3-small

[LLMS.AZURE.AI]
model_endpoint = https://ai-rag410862013562.services.ai.azure.com/models/
model_api_version = 2024-05-01-preview
embedding_api_version = 2024-05-01-preview
models = DeepSeek-R1,Llama-3.2-11B-Vision-Instruct,Llama-3.2-90B-Vision-Instruct
embeddings = text-embedding-3-large

[LLMS.OLLAMA]
MODELTEXT = tinyllama
# Embeddings are used from OPENAI
EMBEDDING_MODEL = text-embedding-3-small

[LLMS.GROQ]
MODELTEXT = gemma2-9b-it
# Embeddings are used from OPENAI
EMBEDDING_MODEL = text-embedding-3-small

[LLMS.NEBUL]
MODELTEXT = Qwen/Qwen3-30B-A3B-Instruct-2507
EMBEDDING_MODEL = Qwen/Qwen3-Embedding-8B
# Nebul API endpoint (OpenAI-compatible)
BASE_URL = https://api.inference.nebul.io/v1
# Available chat/instruct models on Nebul:
#   Qwen/Qwen3-30B-A3B-Instruct-2507  - Qwen3 MoE instruct model (30B total, 3B active)
#   meta-llama/Llama-4-Maverick-17B-128E-Instruct - Meta Llama 4 Maverick MoE instruct model
#   Qwen/Qwen3-VL-235B-A22B-Thinking  - Qwen3 vision-language model with reasoning
#   openai/gpt-oss-120b                - OpenAI open-source 120B model
# Available embedding/retrieval models on Nebul:
#   Qwen/Qwen3-Embedding-8B                       - Qwen3 embedding model, good general purpose
#   intfloat/multilingual-e5-large-instruct        - Multilingual E5 embedding model
#   sentence-transformers/all-MiniLM-L6-v2         - Lightweight sentence embedding model
#   infly/inf-retriever-v1                         - Retrieval-optimized embedding model
MODELS = Qwen/Qwen3-30B-A3B-Instruct-2507,intfloat/multilingual-e5-large-instruct,meta-llama/Llama-4-Maverick-17B-128E-Instruct,sentence-transformers/all-MiniLM-L6-v2,Qwen/Qwen3-VL-235B-A22B-Thinking,infly/inf-retriever-v1,openai/gpt-oss-120b,Qwen/Qwen3-Embedding-8B
EMBEDDINGS = Qwen/Qwen3-Embedding-8B,sentence-transformers/all-MiniLM-L6-v2,infly/inf-retriever-v1,intfloat/multilingual-e5-large-instruct